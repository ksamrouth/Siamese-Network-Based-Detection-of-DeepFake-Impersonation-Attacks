{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pairs preparation and the ResNet50 with Siamese model (with the ResNet50 architecture)"
      ],
      "metadata": {
        "id": "ZoZ6Z3o0kIaI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hg9lZHnbMH6m"
      },
      "outputs": [],
      "source": [
        "#importing the needed libraries\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten, BatchNormalization, Conv2D, MaxPool2D, Activation\n",
        "from keras.models import Sequential, Model\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.metrics import Recall, Precision\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Add, ZeroPadding2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.initializers import glorot_uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qmDhrC45MN-v"
      },
      "outputs": [],
      "source": [
        "def read_jpeg_image(filename):\n",
        "    # Open the image file\n",
        "    img = Image.open(filename)\n",
        "\n",
        "    # Convert the image to a NumPy array\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    return img_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the path of the folder containing the genuine and fake folders of each person\n",
        "path=path_of_folder"
      ],
      "metadata": {
        "id": "7D_dWTsEOj2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDb-jMXig3tQ"
      },
      "source": [
        "To read genuine images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DWuSZna-V_8h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "n = number_of_persons\n",
        "folder_paths = [f\"path\"+\"/person{i}/original\" for i in range(0, n)]\n",
        "original_files = []\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    original_files.append(os.listdir(folder_path))\n",
        "    print(folder_path)\n",
        "\n",
        "for file in original_files:\n",
        "    print(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HKNAbbTg80I"
      },
      "source": [
        "To read fakes images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "loNsZmdxhIZA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "n = number_of_persons\n",
        "folder_paths = [f\"path\"+\"/person{i}/Deepfakes_1\" for i in range(0, n)]\n",
        "deepfakes1_files = []\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    deepfakes1_files.append(os.listdir(folder_path))\n",
        "    print(folder_path)\n",
        "for file in deepfakes1_files:\n",
        "    print(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JYX5KK6ThJZm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "n = number_of_persons\n",
        "folder_paths = [f\"path\"+\"/person{i}/Deepfakes_2\" for i in range(0,n)]\n",
        "deepfakes2_files = []\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    deepfakes2_files.append(os.listdir(folder_path))\n",
        "    print(folder_path)\n",
        "for file in deepfakes2_files:\n",
        "    print(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "n = number_of_persons\n",
        "folder_paths = [f\"path\"+\"/person{i}/Face2Face_1\" for i in range(0, n)]\n",
        "face2face1_files = []\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    face2face1_files.append(os.listdir(folder_path))\n",
        "    print(folder_path)\n",
        "for file in face2face1_files:\n",
        "    print(file)"
      ],
      "metadata": {
        "id": "2HlwzbaGLuep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1H5cHXTkhKlI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "n = number_of_persons\n",
        "folder_paths = [f\"path\"+\"/person{i}/Face2Face_2\" for i in range(0, n)]\n",
        "face2face2_files = []\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    face2face2_files.append(os.listdir(folder_path))\n",
        "    print(folder_path)\n",
        "for file in face2face2_files:\n",
        "    print(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkPUkpEjhMiW"
      },
      "source": [
        "To make the pairs:\n",
        "\n",
        "Genuine pairs consist of two genuine images of a person. Fake pairs consist of two images of a person: one genuine and one fake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D_ltldzZMjMB"
      },
      "outputs": [],
      "source": [
        "#number_of_pairs = number_of_genuine_pairs = number_of_fake_pairs\n",
        "total_sample_size= number_of_pairs\n",
        "path = path_of_folder\n",
        "np = number_of_images\n",
        "n = number_of_persons\n",
        "def get_data( total_sample_size):\n",
        "    #read the image\n",
        "    image = read_jpeg_image(filename='path'+'/person0/original/1.jpg')\n",
        "    #get the size\n",
        "    dim1 = image.shape[0]\n",
        "    dim2 = image.shape[1]\n",
        "    dim3 = image.shape[2]\n",
        "\n",
        "    genuine_count=0\n",
        "\n",
        "    #to initialize the numpy array with the shape of [total_sample_size, 2, dim1, dim2]\n",
        "    x_geuine_pair = np.zeros([total_sample_size, 2, image.shape[0] ,image.shape[1],3])  # 2 for pairs\n",
        "    y_genuine = np.zeros([total_sample_size, 1])\n",
        "    fake_count=0\n",
        "    x_fake_pair = np.zeros([total_sample_size, 2, image.shape[0] ,image.shape[1],3])  # 2 for pairs\n",
        "    y_fake = np.zeros([total_sample_size, 1])\n",
        "#for first set of genuine pairs\n",
        "    for i in range (n):\n",
        "        for j in range (np):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          x_geuine_pair[genuine_count, 0 ] = img1\n",
        "          x_geuine_pair[genuine_count, 1] = img2\n",
        "          y_genuine[genuine_count] = 1\n",
        "          genuine_count+=1\n",
        "    #for first set of fake pairs (using genuine and Deepfakes_1)\n",
        "    for i in range (n):\n",
        "        for j in range (np):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/Deepfakes_1/'+ deepfakes1_files[i][j])\n",
        "          x_fake_pair[fake_count, 0 ] = img1\n",
        "          x_fake_pair[fake_count, 1] = img2\n",
        "          y_fake[fake_count] = 0\n",
        "          fake_count+=1\n",
        "\n",
        "      #for second set of genuine pairs\n",
        "    for i in range (n):\n",
        "        for j in range (np-1):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j+1])\n",
        "          x_geuine_pair[genuine_count, 0 ] = img1\n",
        "          x_geuine_pair[genuine_count, 1] = img2\n",
        "          y_genuine[genuine_count] = 1\n",
        "          genuine_count+=1\n",
        "    #for second set of fake pairs (using genuine and Deepfakes_2)\n",
        "    for i in range (n):\n",
        "        for j in range (np-1):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/Deepfakes_2/'+ deepfakes2_files[i][j])\n",
        "          x_fake_pair[fake_count, 0 ] = img1\n",
        "          x_fake_pair[fake_count, 1] = img2\n",
        "          y_fake[fake_count] = 0\n",
        "          fake_count+=1\n",
        "    #for third set of genuine pairs\n",
        "    for i in range (n):\n",
        "        for j in range (np-3):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j+3])\n",
        "          x_geuine_pair[genuine_count, 0 ] = img1\n",
        "          x_geuine_pair[genuine_count, 1] = img2\n",
        "          y_genuine[genuine_count] = 1\n",
        "          genuine_count+=1\n",
        "    #for third set of fake pairs (using genuine and Face2Face_1)\n",
        "    for i in range (n):\n",
        "        for j in range (np-3):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/Face2Face_1/'+ face2face1_files[i][j])\n",
        "          x_fake_pair[fake_count, 0 ] = img1\n",
        "          x_fake_pair[fake_count, 1] = img2\n",
        "          y_fake[fake_count] = 0\n",
        "          fake_count+=1\n",
        "\n",
        "      #for fourth set of genuine pairs\n",
        "    for i in range (n):\n",
        "        for j in range (np-2):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j+2])\n",
        "          x_geuine_pair[genuine_count, 0 ] = img1\n",
        "          x_geuine_pair[genuine_count, 1] = img2\n",
        "          y_genuine[genuine_count] = 1\n",
        "          genuine_count+=1\n",
        "    #for fourth set of fake pairs (using genuine and Face2Face_2)\n",
        "    for i in range (n):\n",
        "        for j in range (np-2):\n",
        "          img1 = read_jpeg_image('path'+'/person' +str(i)+'/original/'+ original_files[i][j])\n",
        "          img2 = read_jpeg_image('path'+'/person' +str(i)+'/Face2Face_2/'+ face2face2_files[i][j])\n",
        "          x_fake_pair[fake_count, 0 ] = img1\n",
        "          x_fake_pair[fake_count, 1] = img2\n",
        "          y_fake[fake_count] = 0\n",
        "          fake_count+=1\n",
        "\n",
        "    #to concatenate, genuine pairs and fake pairs to get the whole data X and Y\n",
        "    X = np.concatenate([x_geuine_pair, x_fake_pair], axis=0)/255\n",
        "    Y = np.concatenate([y_genuine, y_fake], axis=0)\n",
        "    print(genuine_count)\n",
        "    print(fake_count)\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vsgu9LJmMtUq"
      },
      "outputs": [],
      "source": [
        "#to get X and Y\n",
        "X, Y = get_data(total_sample_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9Q4JTliSMwQm"
      },
      "outputs": [],
      "source": [
        "# to get the training and testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The identity block"
      ],
      "metadata": {
        "id": "hUjbvPQNeeJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the identity block, and this identity block will then be used with convolutional blocks\n",
        "def identity_block(x, f, filters):\n",
        "    #x: input tensor\n",
        "    #f: for kernel size\n",
        "    #filters: number of filters, in a list\n",
        "    #this function returns: x: output of the identity block\n",
        "\n",
        "    #the filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Saving the input value, it will then be added\n",
        "    x_shortcut = x\n",
        "\n",
        "    #Component 1\n",
        "    #Conv2D, Batch normalization, ReLU\n",
        "    x = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=glorot_uniform(seed=0))(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    #Component 2\n",
        "    #Conv2D, Batch normalization, ReLU\n",
        "    x = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    #Component 3\n",
        "    #Conv2D, Batch normalization\n",
        "    x = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=glorot_uniform(seed=0))(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "\n",
        "    #Adding the shortcut value, and passing it through a ReLU activation\n",
        "    #the shortcut and the ReLU activation function\n",
        "    x = Add()([x, x_shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "NDCSp50JdM9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convolutional block"
      ],
      "metadata": {
        "id": "xBkFbVfeeYiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convolutional blocks will then be used with identity blocks in the ResNet50 model\n",
        "def convolutional_block(x, f, filters, s=2):\n",
        "       #x: input tensor\n",
        "       #f: for kernel size\n",
        "       #filters: number of filters, in a list\n",
        "       #s: integer indicating the stride\n",
        "       #this function returns: x: output of the convolutional block\n",
        "    #the filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Saving the input value\n",
        "    x_shortcut = x\n",
        "\n",
        "    #Component 1\n",
        "    #Conv2D, Batch normalization, ReLU\n",
        "    x = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    #Component 2\n",
        "    #Conv2D, Batch normalization, ReLU\n",
        "    x = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    #Component 3\n",
        "    #Conv2D, Batch normalization\n",
        "    x = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=glorot_uniform(seed=0))(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "\n",
        "    #the shortcut path\n",
        "    #Shortcut CONV2D, and shortcut batch normalization\n",
        "    x_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_initializer=glorot_uniform(seed=0))(x_shortcut)\n",
        "    x_shortcut = BatchNormalization(axis=3)(x_shortcut)\n",
        "\n",
        "    #Adding the shortcut value, and passing it through a RELU activation\n",
        "    #the shortcut and the ReLU activation function\n",
        "    x = Add()([x, x_shortcut])\n",
        "    # ReLU\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "HXF1o0RtdM4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ResNet50 model"
      ],
      "metadata": {
        "id": "6YZ9rGiTeU2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet50_model(input_shape):\n",
        "\n",
        "    #convolutional layer, Max Pooling layer, convolutional blocks and identity blocks, Average Pooling layer, and Output layer\n",
        "\n",
        "    # Defining the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "    # ZeroPadding\n",
        "    x = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    #the stage 1\n",
        "    #convolutuional layer, Batch normalization, ReLU\n",
        "    x = Conv2D(64, (7, 7), strides=(2, 2), kernel_initializer=glorot_uniform(seed=0))(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    # Max Pooling layer\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "\n",
        "    #the stage 2\n",
        "    #the convolutional block\n",
        "    x = convolutional_block(x, f=3, filters=[64, 64, 256], s=1)\n",
        "    # 2 identity blocks\n",
        "    x = identity_block(x, 3, [64, 64, 256])\n",
        "    x = identity_block(x, 3, [64, 64, 256])\n",
        "\n",
        "\n",
        "\n",
        "    #the stage 3\n",
        "    #the convolutional block\n",
        "    x = convolutional_block(x, f=3, filters=[128, 128, 512], s=2)\n",
        "    # 3 identity blocks\n",
        "    x = identity_block(x, 3, [128, 128, 512])\n",
        "    x = identity_block(x, 3, [128, 128, 512])\n",
        "    x = identity_block(x, 3, [128, 128, 512])\n",
        "\n",
        "\n",
        "    #the stage 4\n",
        "    #the convolutional block\n",
        "    x = convolutional_block(x, f=3, filters=[256, 256, 1024], s=2)\n",
        "    # 4 identity blocks\n",
        "    x = identity_block(x, 3, [256, 256, 1024])\n",
        "    x = identity_block(x, 3, [256, 256, 1024])\n",
        "    x = identity_block(x, 3, [256, 256, 1024])\n",
        "    x = identity_block(x, 3, [256, 256, 1024])\n",
        "\n",
        "\n",
        "    #the stage 5\n",
        "    #the convoultional block\n",
        "    x = convolutional_block(x, f=3, filters=[512, 512, 2048], s=2)\n",
        "    # 2 identity blocks\n",
        "    x = identity_block(x, 3, [512, 512, 2048])\n",
        "    x = identity_block(x, 3, [512, 512, 2048])\n",
        "\n",
        "    #Average Pooling layer\n",
        "    x = AveragePooling2D(pool_size=(7, 7), padding='same')(x)\n",
        "    #the output layer\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='softmax')(x)\n",
        "    # to create the Model\n",
        "    model = Model(inputs=X_input, outputs=x, name='ResNet50')\n",
        "\n",
        "    # returning ResNet50 as model\n",
        "    return model"
      ],
      "metadata": {
        "id": "lSjmOrfddM0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1YXLoBukM_oT"
      },
      "outputs": [],
      "source": [
        "#to get the input shape\n",
        "input_shape=x_train.shape[2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IurQI4ChNGaY"
      },
      "outputs": [],
      "source": [
        "#base_network will be used in the siamese model\n",
        "base_network = ResNet50_model(input_shape=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mc0ywiSiNGvW"
      },
      "outputs": [],
      "source": [
        "#the siamese model\n",
        "from keras.layers import concatenate\n",
        "img_a =  Input(shape=input_shape)\n",
        "img_b =  Input(shape=input_shape)\n",
        "img_a_feat = base_network(img_a)\n",
        "img_b_feat = base_network(img_b)\n",
        "#combined features\n",
        "combined_features = concatenate([img_a_feat, img_b_feat], name = 'merge_features')\n",
        "#dense layers, batchNormalization and sigmoid\n",
        "combined_features = Dense(100, activation = 'relu')(combined_features)\n",
        "combined_features = Activation('relu')(combined_features)\n",
        "combined_features = Dense(20, activation = 'relu')(combined_features)\n",
        "combined_features = BatchNormalization()(combined_features)\n",
        "combined_features = Dense(1, activation = 'sigmoid')(combined_features)\n",
        "similarity_model = Model(inputs = [img_a, img_b], outputs = [combined_features], name = 'Similarity_Model')\n",
        "similarity_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DZFODoZjM5YH"
      },
      "outputs": [],
      "source": [
        "# to compile the siamese model\n",
        "similarity_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy',tf.metrics.Recall(), tf.metrics.Precision()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w7U0BiHuNKzK"
      },
      "outputs": [],
      "source": [
        "#the inputs for the siamese model\n",
        "img_1 = x_train[:, 0]\n",
        "img_2 = x_train[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mfuQ-rYtNM5x"
      },
      "outputs": [],
      "source": [
        "#to train the siamese model\n",
        "loss_history = similarity_model.fit([img_1, img_2], y_train,\n",
        "          batch_size=10, verbose=2, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ksqLODQNSEG"
      },
      "outputs": [],
      "source": [
        "#to evaluate the siamese model\n",
        "pred = similarity_model.evaluate([x_test[:, 0], x_test[:, 1]], y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NH-aGxa0NTxJ"
      },
      "outputs": [],
      "source": [
        "#for siamese model prediction\n",
        "pred = similarity_model.predict([x_test[:, 0], x_test[:, 1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5WlC-J79Ncgy"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JcuJvAWeNecL"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "74Jd6w_Lgpog"
      },
      "outputs": [],
      "source": [
        "#importing the needed library\n",
        "from sklearn .metrics import roc_auc_score\n",
        "auc = np.round(roc_auc_score(y_test, pred), 3)\n",
        "print(\"Auc for our sample data is {}\".format(auc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4M5wV7Xmgpoh"
      },
      "outputs": [],
      "source": [
        "#importing the needed libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U--_5YyLgpoi"
      },
      "outputs": [],
      "source": [
        "# to calculate the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "# to plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}